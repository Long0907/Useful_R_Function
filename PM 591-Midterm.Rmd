---
title: "PM 591- MIDTERM"
author: "Luqing Ren"
date: "3/11/2021"
output: html_document
---

#### Read carefully before jumping in. Answer all questions and include all R code in this Markdown file. Submit both a copy of the Markdown file and the html file generated by knitR.
<br>

##### 1. (65 points). 
##### Condition X is a neurological disease that can lead to rapid and severe cognitive decline if left untreated. Although there is no known definitive cure for condition X, early treatment with drug A can dramatically reduce the speed of cognitive decline. However, drug A substantially increseases the risk of  hemorrhagic stroke, which is potentially deadly. Early symptoms of condition X are virtually indistinguishable from those of the far more common condition Z, but an accurate diagnosis of condition X that rules out condition Z requires an invasive and costly procedure. 

##### A study has been conducted to develop and evaluate a new potential screening tool for individuals with early symptoms of conditions X and Z to discriminate between the two. The screening only requires measurements of 3 blood biomarkers. 

##### The csv file ``ConditionX.csv`` contains the biomarker as well as personal and demographic data from the study on 655 patients diagnosed with condition X and 713 patients diagnosed with condition Z. 

```{r,include=FALSE}
library(tidyverse)
library(FNN)
library(pROC)
library(vcd)
library(MASS)
```

```{r}
classificationError<- function(a) {
  error = (a[1,2]+a[2,1])/sum(a)
  sens= a[2,2]/(a[2,1]+a[2,2])
  spec=a[1,1]/(a[1,1]+a[1,2])
  return(c(error=error,sensitivity=sens, specificity= spec))
}
```

a. (35 points) Develop a screening tool to discriminate between conditions X and Z using all 6 available features (the 3 personal/demographic and the 3 biomarker features). Consider at least two appropriate machine learning methods and select the best using an appropriate approach.
```{r}
conj<- read_csv("ConditionX(1).csv")
conj<-
  conj %>% 
  mutate(gender= factor(gender, levels=c("M","F"),labels = c("1","2")),
         condition= factor(condition,levels=c("Z","X"),labels = c("Z","X")))
# split data set
set.seed(200)
n<- nrow(conj)
train<- sample(1:n, floor((0.7)*n))
conj_train<- conj[train,] 
conj_vali<- conj[-train,]

#logistic 
conj_glm <- glm(condition ~.,family= "binomial",data= conj)
pred_glm_train <- factor(predict(conj_glm, conj_train,type = 'response') > 0.5)
levels(pred_glm_train) = c("Z","X")
confMaxtrix_glm_train <- table(true = conj_train$condition,predicted = pred_glm_train)
error_glm_train<-classificationError(confMaxtrix_glm_train)
error_glm <-error_glm_train[1]
error_glm

pred_glm_vali <- factor(predict(conj_glm, conj_vali,type = 'response') > 0.5)
levels(pred_glm_vali) = c("Z","X")
confMaxtrix_glm_vali <- table(true = conj_vali$condition,predicted = pred_glm_vali)
error_glm_vali<-classificationError(confMaxtrix_glm_vali)
error_glm_validation <-error_glm_vali[1]

error_glm_validation
#LDA classifiers
conj_lda <- lda(condition ~.,data= conj)
pre_lda_train <- predict(conj_lda,conj_train)
confMatrix_lda_train <- table(true= conj_train$condition,predicted= pre_lda_train$class)
error_train<-classificationError(confMatrix_lda_train)
error_train_lad <-error_train[1]
error_train_lad
pre_lda_vali <- predict(conj_lda,conj_vali)
confMatrix_lda_vali <- table(true= conj_vali$condition,predicted= pre_lda_vali$class)
error_vali<-classificationError(confMatrix_lda_vali)
error_vali_lad <-error_vali[1]
error_vali_lad


```

The LDA method and logistic method have the same validation perfprmance error. So I pick the logistic classifier.

b. (15 points) For the method selected, compute the test confusion matrix using a probability cutoff = 0.5, the misclassification error, the sensitivity and the specificty (Note: if your best model does not return probabilities use the second best model for parts b,c and d). Based on the description of the problem above, describe the harm associated with miss-diagnosing a patient with condition Z as having condition X and miss-diagnosing a patient with condition X as having condition Z. 
```{r}
#logistic 
conj_glm <- glm(condition ~.,family= "binomial",data= conj)
pred_glm_train <- factor(predict(conj_glm, conj_train,type = 'response') > 0.5)
levels(pred_glm_train) = c("Z","X")
confMaxtrix_glm_train <- table(true = conj_train$condition,predicted = pred_glm_train)
error_glm_train<-classificationError(confMaxtrix_glm_train)
error_glm <-error_glm_train
error_glm 

pred_glm_vali <- factor(predict(conj_glm, conj_vali,type = 'response') > 0.5)
levels(pred_glm_vali) = c("Z","X")
confMaxtrix_glm_vali <- table(true = conj_vali$condition,predicted = pred_glm_vali)
error_glm_vali<-classificationError(confMaxtrix_glm_vali)
error_glm_validation <-error_glm_vali
error_glm_validation
```

High sensitivity screening is needed for condition X, since it is virtually indistinguishable at early state. A high specifivity screening is good for condition Z,since it is invasive and costly procedure.

c. (10 points) Plot the test ROC curve and report the test AUC for the model selected. Assuming a new screening tool for condition X would be considered acceptable if it has at least 90% sensitivity and 50% specificity, is your screening tool acceptable? 
```{r}
predicted_prob_glm <- predict(conj_glm,newdata=conj_vali)
roc_glm <- roc(conj_vali$condition,predicted_prob_glm,ci=TRUE,of='auc')
auc(roc_glm)

plot(roc_glm,lwd=4,col='red4')
```


when the sensitivity is 90%, the specificity is around 50% from the ROC curve, so the screening tool is acceptable.


d) (5 points) The file ``ConditionZ_new.csv`` contains the age, gender, bmi and the 3 biomarkers measured on 10 new patients with early symptoms for conditions Z and X. Screen these patients for condition X or Z using the screening tool you developed in a) (you can use the probability cutoff = 0.5)
```{r}
newconj<- read_csv("ConditionX_new(1).CSV")

newconj<-
  newconj %>% 
  mutate(gender= factor(gender, levels=c("M","F"),labels = c("1","2")))

predicted_new <- predict(conj_glm,newdata=newconj)

newconj<-
  newconj %>% 
  mutate(predict= case_when (predicted_new <0.5 ~"Z",
                         predicted_new >0.5 ~"X")
         )
newconj
```

<br>

##### 2. (35 points total. 3.5 points each). 
##### Decide whether each statement below is true or false and provide a short explanation. Keep in mind that the explanation is the most important part of your answer. 

i. A knn regression model with a smaller k is more likely to overfit than a knn regression model with a larger k. (TRUE)

-- KNN regression predict a point by average the distance over the nearest k points. A smaller K value, such as k=1, the model will fit every points which causes overfit result. On the other hand, when k value is equal to n, the model predicts the average y value over entire training data,the decision boundary will be a constant line which causes underfit result. Therefore, a small er k is more likely to overfit than larger k.

ii. Computational costs aside, a leave one out cross-validation estimate of prediction performance (e.g. mse, $R^2$) is better than a 5-fold or 10-fold cross-validation estimate of prediction performance. (TRUE)

--In LOOCV, we only leave one observation and train the model on n-1 observations. It uses n-1 subjects to train the model each time. As a result, the LOOCV is more variable. In 5-fold or 10-fold CV, the training sets are less than the LOOCV,therefore, k-fold CV overestimates error,but it is less variable.


iii. A regression model has low variance if it approximates well the true regression function on average when fitted to training data. (FALSE)

--Variance represents how variable the estimated regression function is across the training sets. If  a regression model approximates well the true regression function on average, this regression model is low bias rather than low variance.


iv. Given a fixed overall sample size, setting aside a larger test set would yield a less variable estimate of performance on new data.(FALSE)

--In a fixed sample size, a larger test set means a smaller training set.  A smaller training set has a high variance. Therefore, the new model fit would has a high variable.


v. Given a fixed overall sample size, setting aside a larger training set would yield a regression model with lower variance. (TRUE)

--When training sample size increases, the estimated of performance error will decrease as the estimated becomes more precise. As a result, the regression model has a lower variance. 


vi. To make good predictions using linear regression the underlying 'true model' has to be linear and the errors have to be normally distributed. (FALSE)

--In ML for prediction, the linearity, normal distribution and equal variance assumptions are not needed when using linear regression.


vii. Although the estimate of prediction performance based on the training data is biased for estimating performance on new data, it is ok to use the prediction performance based on the training data for comparing different models to choose the best. (FALSE)

--We can not compare models based on the training performance error, because more complex model will always have smaller training error. A validation set is needed to perform model selection.

viii. If the size of the validation set is too small, model selection maybe unreliable when using a single training/validation split. (TRUE)

--A small size of validation set may cause the estimated of prediction performance high variance especially using a random single training/validation split. The range of prediction performance vary substantially, so it is not reliable to select model. A k-fold cross-validation or repeated k-fold cross-validation is better when the validation set is too small.


ix.  A cross-validated measure of performance can be used to select the best performing model but evaluation on a test set is still required to get an honest assessment of performance on new data. (TRUE)

--We fit the best model and choose the best model by using least square approach, a test set is required to get an unbiased final estimated of performance. Performance of test set indicates how well the model is when it is applied on new data.

x. Overfitting happens when a machine learning model is too flexible for the particular prediction problem at hand and for the amount of available training data.(TRUE)

--If a model captures the noise rather than the true trend in the training data, the model is overfitting.
A flexible model has high capacity to capture the trend in training data, but also capture the noise in data set. therefore, the more complex model is more likely overfitting. 



