---
title: "Classification and Regression Trees"
output:
  html_document:
    df_print: paged
---
Learning objectives.

- Perform classification and regression with tree-based methods in R
- Recognize that tree-based methods are capable of capturing non-linearities by splitting multiple times on the same variables

### Analysis/conceptual
You will assess how well a tree model can capture non-linearities by fitting a regression tree to simulated non-linear data.

i. Simulate the data

```{r, eval=FALSE}
set.seed(1984) 
n = 1000
x = runif(n, -5, 5) # n observations uniformly distributed in the interval -5 to 5
error = rnorm(n, sd=0.5)
y = sin(x) + error # nonlinear relationship between outcome y and feature x
nonlin = data.frame(y=y, x=x)
```

ii. Split the data into training and testing (500 observations in each). Plot the data -- scatter plot of y vs. x

```{r}
set.seed(1984)
index<- sample(1:nrow(nonlin),floor(0.5*nrow(nonlin)))
non_train<- nonlin[index,]
non_test<- nonlin[-index,]

# scatter plot y vs.x
par(mar=c(5,4,2,2))
plot(x=nonlin$x,y=nonlin$y, xlab=" feature x", ylab="sin(x)+error")
```

iii. Fit a __regression tree__ using the training set 

```{r, eval=FALSE}
library(rpart) 
treefit = rpart(y~x, method='anova', control=list(cp=0), data=non_train) 
# Method='anova' indicate a regression tree. cp=0 ensures that binary recursive partitioning will not stop early due to lack of improvement in RSS by an amount of at least cp
```

iv. Plot the fitted regression tree

```{r, eval=FALSE}
plot(treefit) # plots the tree
text(treefit) # annotates the tree. May fail if tree is too large

library(rattle)
fancyRpartPlot(treefit) #the function fancyRpartPlot in package rattle draws good looking trees!
```

v. Plot the cv relative error to determine the optimal complexity parameter

```{r, eval=FALSE}
plotcp(treefit)
```

vi. Print the table complexity parameter values and their associated cv-errors

```{r, eval=FALSE}
printcp(treefit)
```

vii. Select the optimal complexity parameter and prune the tree

```{r, eval=FALSE}
optimalcp =  0.023
treepruned = prune(treefit, cp=optimalcp)
```

viii. Plot the pruned tree
```{r}
fancyRpartPlot(treepruned)
```


ix. Summarize the pruned tree object and relate the summary to the plotted tree above

```{r, eval=FALSE}
summary(treepruned)
```

x. Based on the plot and/or summary of the pruned tree create a vector of the (ordered) split points for variable x, and a vector of fitted values for the intervals determined by the split points of x.

```{r, eval=FALSE}
x_splits = sort(c(-3.7,-0.2,-2.9,3.1,3.5,0.62,2.4),decreasing = FALSE)
y_splits = sort(c(-0.81,0.098,-0.92,-0.19,0.22,0.29,0.96,0.93),decreasing = FALSE)
```

xi. Plot the step function corresponding to the fitted (pruned) tree

```{r, eval=FALSE}
plot(y~x, data=non_train)
stpfn = stepfun(x_splits, y_splits) #stepfun() creates the step function 
plot(stpfn, add=TRUE, lwd=2, col='red4') #add=TRUE plots over the existing plot 
```

xii. Fit a linear model to the training data and plot the regression line. Contrast the quality of the fit of the tree model vs. linear regression by inspection of the plot

```{r, eval=FALSE}
lmfit = lm(y ~ x, data=non_train)
summary(lmfit)

plot(y~x, data=non_train)
plot(stpfn, add=TRUE, lwd=2, col='red4')
abline(lmfit, col='blue', lwd=2)
legend(leftbottom)
```
 
xiii. Compute the test MSE of the pruned tree and the linear regression model
```{r}
treepred<- predict(treepruned, data= non_test)
test_MSE <- mean((treepred-treepruned$y)^2)
print(paste("The mean of test MSE of the pruned tree is" ,round(test_MSE,3)))

lmpred = lm(y ~ x, data=non_test)
summary(lmpred)
```


### Analysis
You will recreate the analysis of the heart data in the textbook and lecture. 
```{r,including=FALSE}
library(tidyverse)
heart<- read_csv("Heart.csv")

# convert variables into factors
heart$Sex <- as.factor(heart$Sex)
heart$ChestPain <- as.factor(heart$ChestPain)
heart$Fbs<- as.factor(heart$Fbs)
heart$RestECG<- as.factor(heart$RestECG)
heart$ExAng<- as.factor(heart$ExAng)
heart$Slope<- as.factor(heart$Slope)
heart$Ca<- as.factor(heart$Ca)
heart$Thal<- as.factor(heart$Thal)
heart$AHD<- factor(heart$AHD,
                   levels = c("No", "Yes"), 
                   labels = c("0", "1"))  

# impute the missing value
heart<- rfImpute(AHD~., heart, iter=6)
```

i.   Split the data into training and testing
```{r}
set.seed(1984)
heart_index<- sample(1:nrow(heart),floor(0.7*nrow(heart)))
heart_train<- heart[heart_index,]
heart_test<- heart[-heart_index,]
```

ii.  Fit a classification tree using ``rpart``
```{r}
heart_tree<- rpart(AHD~.,data=heart_train, method="class",control=list(minsplit=15, minbucket=5,cp=0))
printcp(heart_tree)
```

iii. Plot the un-pruned tree
```{r}
plot(heart_tree) # plots the tree

library(rattle)
fancyRpartPlot(heart_tree) #the function fancyRpartPlot in package rattle draws good looking trees!
```

iv.  Plot the cv error
```{r}
plotcp(heart_tree)
cp = heart_tree$cptable[which.min(heart_tree$cptable[,"xerror"]),"CP"]

print(paste("The cp value is", cp, "having the least cv error"))
```

v.  Prune the tree using the optimal complexity parameter
```{r}
heart_tree_pruned <- prune(heart_tree, cp=0)
heart_tree_pruned
```

vi. Plot the pruned tree
```{r}
fancyRpartPlot(heart_tree_pruned,cex=0.6,yesno=2)
```
 
vii. Compute the test misclassification error
```{r}
heart_treepredict<- predict(heart_tree_pruned, data= heart_test[,-1], type="class")
table(heart_treepredict)

with(heart_test,table(heart_treepredict, AHD))

```


vii. Fit the tree with the optimal complexity parameter to the full data (training + testing)

```{r}
treefit_heart <-  rpart(AHD~.,data=heart, method="class",control=list(minsplit=15, minbucket=5, cp=0))
printcp(treefit_heart)
```



